# LLM API Call Optimization Summary

## âœ… Current Optimizations in Place

### 1. **Pre-Tokenizer Module** (READY but NOT INTEGRATED)
Located in `/pre_tokenizer/` - a standalone module that:
- **Deduplicates samples** based on normalized patterns (removes redundant logs)
- **Optimizes token usage** by selecting most diverse samples within budget
- **Accurate token counting** for Claude, GPT-4, and other models
- **Pattern recognition** for 20+ log format types

**STATUS**: Built but needs integration into `src/llm_iterative_session.py`

### 2. **Rate Limiting & Cost Controls**
- Max iterations reduced from 5 to 2 (60% cost reduction)
- Minimal delays between calls (1-3 seconds instead of exponential backoff)
- Cost tracking and credit balance display

### 3. **Real Log Data** 
- 782,375 real log entries from LogHub (Apache, SSH, OpenStack)
- Converted to DFE schema with all metadata fields
- No synthetic data (following CLAUDE.md guidelines)

## ðŸš€ Additional Optimizations to Implement

### 1. **INTEGRATE PRE-TOKENIZER** (High Priority)
```python
# In src/llm_iterative_session.py
from pre_tokenizer import PreTokenizer

# Before sending samples to LLM:
tokenizer = PreTokenizer(max_tokens=50000)  # Leave room for prompts
result = tokenizer.prepare_for_llm(samples)
optimized_samples = result['samples']  # Use these instead of raw samples
```
**Impact**: 50-80% reduction in token usage while maintaining pattern coverage

### 2. **PROMPT COMPRESSION**
- Remove verbose instructions after first iteration
- Use abbreviated error codes instead of full descriptions
- Compress VRL guidance to essential rules only after initial attempt

**Example**:
```python
if iteration > 1:
    # Use compressed prompt with just error fixes
    prompt = self._build_compressed_feedback_prompt(errors)
else:
    # Full initial prompt with all guidance
    prompt = self._build_initial_prompt(samples)
```
**Impact**: 30-40% prompt size reduction on iterations 2+

### 3. **SMART SAMPLE SELECTION**
Instead of sending all samples, categorize and send representatives:
```python
def select_representative_samples(samples):
    # Group by detected pattern
    grouped = {}
    for sample in samples:
        pattern = detect_log_pattern(sample)
        if pattern not in grouped:
            grouped[pattern] = []
        grouped[pattern].append(sample)
    
    # Take 2-3 examples per pattern max
    selected = []
    for pattern, pattern_samples in grouped.items():
        selected.extend(pattern_samples[:3])
    
    return selected
```
**Impact**: 60-90% reduction in sample data sent

### 4. **CACHING SUCCESSFUL PATTERNS**
```python
# Cache successful VRL patterns for similar log types
cache_file = "vrl_pattern_cache.json"

def get_cached_vrl_pattern(log_type):
    if os.path.exists(cache_file):
        cache = json.load(open(cache_file))
        return cache.get(log_type)
    return None

def cache_successful_vrl(log_type, vrl_code):
    cache = {}
    if os.path.exists(cache_file):
        cache = json.load(open(cache_file))
    cache[log_type] = vrl_code
    json.dump(cache, open(cache_file, 'w'))
```
**Impact**: Skip LLM entirely for previously solved patterns

### 5. **STREAMING RESPONSES**
For Anthropic API:
```python
# Use streaming to get faster initial feedback
with client.messages.stream(
    model=model,
    messages=messages,
    max_tokens=max_tokens,
    stream=True
) as stream:
    for text in stream.text_stream:
        # Process VRL as it arrives
        if "```vrl" in text:
            # Start extracting/validating early
```
**Impact**: 20-30% perceived speed improvement

### 6. **PARALLEL VALIDATION**
```python
import asyncio

async def validate_vrl_async(vrl_code):
    # Run PyVRL and Vector CLI validation in parallel
    pyvrl_task = asyncio.create_task(validate_with_pyvrl(vrl_code))
    vector_task = asyncio.create_task(validate_with_vector(vrl_code))
    
    pyvrl_result = await pyvrl_task
    vector_result = await vector_task
    
    return pyvrl_result, vector_result
```
**Impact**: 40-50% faster validation phase

### 7. **FOCUSED ERROR FEEDBACK**
Instead of sending all errors, prioritize:
```python
def prioritize_errors(errors):
    # Critical errors that must be fixed
    critical = ['E103', 'E110', 'E651']  # VRL-specific errors
    
    # Sort errors by criticality
    priority_errors = []
    other_errors = []
    
    for error in errors:
        if any(code in error for code in critical):
            priority_errors.append(error)
        else:
            other_errors.append(error)
    
    # Send only top 5 most critical
    return priority_errors[:5]
```
**Impact**: Clearer feedback, faster convergence

### 8. **MODEL SELECTION OPTIMIZATION**
```python
def select_optimal_model(sample_count, complexity):
    # Use cheaper models for simple tasks
    if sample_count < 10 and complexity == "low":
        return "claude-3-haiku"  # Fastest, cheapest
    elif sample_count < 50:
        return "claude-3-sonnet"  # Good balance
    else:
        return "claude-3-opus"  # Most capable for complex
```
**Impact**: 30-70% cost reduction on simple parsers

## ðŸ’° Estimated Overall Impact

With all optimizations:
- **Token Usage**: 70-85% reduction
- **API Costs**: 60-80% reduction  
- **Processing Time**: 40-60% faster
- **Success Rate**: 20-30% improvement (better focused prompts)

## ðŸ“‹ Implementation Priority

1. **Integrate Pre-Tokenizer** (Immediate - biggest impact)
2. **Smart Sample Selection** (High - major token savings)
3. **Prompt Compression** (High - easy win)
4. **Focused Error Feedback** (Medium - improves convergence)
5. **Caching** (Medium - helps with repeated patterns)
6. **Parallel Validation** (Low - nice speedup)
7. **Streaming** (Low - UX improvement)
8. **Model Selection** (Low - cost optimization)

## ðŸ”§ Quick Implementation

The most impactful change you can make RIGHT NOW:

```python
# In src/vrl_testing_loop_clean.py or wherever samples are loaded:

from pre_tokenizer import PreTokenizer

# Add this before calling LLM:
def optimize_samples_for_llm(samples):
    tokenizer = PreTokenizer(max_tokens=30000)  # Conservative limit
    result = tokenizer.prepare_for_llm(samples)
    
    logger.info(f"Optimized {len(samples)} samples to {result['count']}")
    logger.info(f"Token usage: {result['optimization_stats']['total_tokens']}")
    logger.info(f"Pattern coverage: {result['optimization_stats']['pattern_coverage']}")
    
    return result['samples']

# Then use:
optimized_samples = optimize_samples_for_llm(raw_samples)
# Pass optimized_samples to LLM instead of raw_samples
```

This alone should reduce your API costs by 70-80% while maintaining full pattern coverage!